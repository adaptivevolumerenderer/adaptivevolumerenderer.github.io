<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Adaptive Volume Renderer for Neural Radiance Field">
  <meta name="keywords" content="AVR, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Combining Volume Rendering with Adaptive Ray Marching in Neural Radiance Fields</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Combining Volume Rendering with Adaptive Ray Marching in Neural Radiance Fields</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Yanke Song<sup>1</sup>,</span>
            <span class="author-block">
              Zelin Li<sup>2</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Harvard University,</span>
            <span class="author-block"><sup>2</sup>MIT Sloan School of Management</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/yankeesong/adaptive-volume-rendering"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          
          <p>
          In this project, we explore a more efficient way of volume rendering given a neural scene representation. 
          In specific, given a locally conditioned Neural Radiance Field introduced in PixelNeRF [6], we replace 
          the evenly sampled points in their volume rendering procedure with adaptively sampled points to reduce
          the time cost associated with the procedure. We obtain adaptively sampled points by first predicting the approximate 
          intersection between the query ray and the scene via the differentiable ray-marching technique [4] using LSTM, and 
          then evenly sample points with tiny steps. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>
  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Related Work. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Related Work</h2>
        <div class="content has-text-justified">
          
          <p>
            Our work is inspired by recent work in the field that focuses on predicting neural scene representation with one or few input images. 
            We review the related work behind the key techniques in our project.
          </p>
          
          <p>
            <strong>Local Conditioning on Neural Radiance Fields</strong> PixelNeRF [6] augments Neural Radiance Field (NeRF) [2], 
            a 3D scene representation algorithm based on neural network, by including a fully convolutional 
            image encoder to learn spatial image features that can be shared across scenes, as well as the local-conditioning technique 
            that extract the local features by projecting the query points onto input images. 
            Another line of work [1, 3] introduced similar techniques with different way 
            of aggregating local features, and they refer this kind of technique as Wrap-Conditioned Embedding (WCE). 
            Reizenstein et al. [3] also incorporates WCE into Scene Representation Network (SRN) [4], 
            which showed a decent improvement in performance than the vanilla SRN and actually resulted in the best depth 
            estimation results in their experiments.
          </p>
          
          <p>
            <strong>Aggregating projected features with Transformer</strong> When we apply WCE to multi-view inputs, we need to 
            aggregate local features extracted from multiple query points on multiple views. PixelNeRF simply process each 
            query point independently, and aggregate by averaging over multiple views. NerFormer instead uses a Transformer 
            to process these features and produced better results. They argued that a Transformer (1) can learn to aggregate 
            features from multiple views in a smart way and (2) is capable of spatial reasoning on multiple query points.
          </p>

          <p>
            <strong>Differentiable Ray Marching</strong> Both PixelNeRF [6] and NerFormer [3]
            use volume rendering with evenly spaced points along the query ray, which might contain many points in empty spaces. 
            Since volume rendering is a slow explicit rendering technique, this could result in wasteful computational resources. 
            On the other hand, SRN [4] uses a differentiable ray marching technique, which employs an LSTM to adaptively 
            sample points along the query ray until it finds the intersection of the query ray with the scene. SRN then pass 
            this intersection point through a pixel generator network that directly yields the color, instead of using volume rendering. 
            Reizenstein et al. [3] shows that NerFormer performs better than SRN+WCE in many categories, 
            possibly suggesting that volume rendering, although more expensive, produces better results than ray marching.
          </p>

        </div>
      </div>
    </div>
    <!--/ Related Work. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          
          <p>
            We propose an adapted version of differentiable ray marching algorithm that will first use ray marching long short-term memory (RM-LSTM) 
            along the query ray to learn the adaptive step-length for the tracing algorithm. The goal of the RM-LSTM algorithm is to land in a 
            neighborhood to the actual surface of the scene in order for the neural radiance field to learn the output. As an extension, we can 
            sample points starting from that approximate intersection point predicted by the RM-LSTM algorithm with much smaller step sizes and 
            perform volume rendering along those points. 
          </p>
          
          <p>
            We did our experiment by modifying the current structure of PixelNeRF. Instead of a standard volume rendering procedure, we extract the
            latent features learned by the CNN encoder of PixelNeF and then feed the features as input into the RM-LSTM algorithm in the picture below.
            Essentially, the RM-LSTM predicts the steplength on the ray of interest and update the depth. The RM-LSTM is set to have the number of feature 
            channels being 512. We set the number of steps to be 10 in our experiments due to computational constraints.  
            We then pass the output through the whole PixelNeRF network again to obtain rgb values. 
            In order to guide the initial convergence process of LSTM, we add a regularization term to the usual MSE reconstruction loss by penalizing any final
            depth outside 0.5 (near plane) and 2.0 (far plane).
            In our experiments, we use pretrained ResNet 34 model for the CNN encoder. We denote this structure as Raymarcher.
          </p>
          
          <p>
            For our extension, we did the same procedure as before up to the point where LSTM outputs the final depth. Now instead of extracting rgb values 
            from PixelNeRF on this single point, we sample n = 10 points around +/-0.15 neighborhood of predicted depth and use standard volume rendering process to
            calculate the final rgb value. We denote this structure as AdaptiveVolumeRenderer (AVR).
          </p>
          <img src = "https://drive.google.com/uc?export=view&id=1BleKMk4jOASjC4X5Np7CbZgqsPXfD5rh" alt = "Model Architecture">
        </div>
      </div>
    </div>
    <!--/ Method. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          
          <p> 
            In our experiment, we compare our two models, Raymarcher and Adaptive Volume Renderer, against a baseline that uses standard volume rendering
            with PixelNeRF, where we take the pretrained weights and fine-tune for 40k steps. The number of coarse points we sample for this process is set to be 32, 
            and we do not utilize fine points sampling as implemented by the original PixelNeRF repository given time constraints. We train Raymarcher on top
            of the final weights from baseline for 400k steps. We further train Adaptive Volume Renderer on top of the final weights from Raymarcher for 40k steps.
          </p>
          
          <p>
            We evaluate our models on ShapeNet cars dataset, where the training set contains 2150 cars with resolution 32 and 8 views per scene, 1 of them randomly
            selected as the input/source view. We then test on 352 cars for single-view reconstruction, using an informative input view (view 64). 
            We compare the performance using PSNR, SSIM [5], the rendering speed, and videos of produced output. The results are shown below.
            
          </p>

          <div class="row">
            <div class="column">
              <div class="column content">
                <p>
                  <strong>Volume Rendering (baseline)</strong>
                </p>
               <img src = "https://drive.google.com/uc?export=view&id=11qAz84kdEFOTFVWqgUPcr_seqRr6HWRE" 
                    alt = "Volume Rendering Output (Baseline)"
                    width = "200"
                    height = "200"
                    class="center">
              </div>
            </div>

            <div class="column">
              <div class="column content">
                <p>
                  <strong>Raymarcher</strong>
                </p>
               <img src = "https://drive.google.com/uc?export=view&id=1UBhqsxIq26Ti2nc-W_VyEmgdr3AnPQTQ" 
                    alt = "Raymarcher Output"
                    width = "200"
                    height = "200"
                    class="center">
              </div>
            </div>

            <div class="column">
              <div class="column content">
                <p>
                  <strong>Adaptive Volume Renderer</strong>
                </p>
               <img src = "https://drive.google.com/uc?export=view&id=1xWMSWUNbHTq2srFG2N08QXzFJZxlR5of" 
                    alt = "AVR Output"
                    width = "200"
                    height = "200"
                    class="center">
              </div>
            </div>
             
          </div>
             
            <p>
              As we can see, Raymarcher has comparable results with the standard Volume Rendering process, but has rendering speed ~4x faster.
              Adaptive Volume Renderer outperforms both Raymarcher and Baseline on both PSNR and SSIM and has rendering speed ~2x faster comparing
              againast baseline. 
             </p>
          </div>
          
        </div>
      </div>
    </div>
    <!--/ Results. -->
  </div>
</section>
  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Next Steps. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">High Resolution Comparison</h2>
        <div class="content has-text-justified">
          
          <p>
            For the full 128 by 128 resolution, we compared both Raymarcher and AVR with the fine + coarse sampling version of Volume Rendering in PixelNeRF.
            For VR, we directly used PixelNeRF's pretrained weights. We initialized the raymarcher for 10 steps, and for AVR we sampled 20 points around +/-0.15 neighborhood
            around the final depth from Raymarcher. We trained both the Raymarcher and AVR together from the pretrained weights of PixelNeRF. We trained with a batch size
            of 4 scenes, sampling 512 rays randomly from all training views for each scene. We trained the model for 300 epochs (183000 iterations).
            See the results below: 
          </p>
         
        </div>
      </div>
    </div>
    <!--/ Next Steps. -->
  </div>
</section>

  
<section class="section">
  <div class="container is-max-desktop">
    <!-- References. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">References</h2>
        <div class="content has-text-justified">
          <ol>
            <li>Philipp Henzler, Jeremy Reizenstein, Patrick Labatut, Roman Shapovalov, Tobias Ritschel, Andrea Vedaldi, and David Novotny. 
              Unsupervised learning of 3d object categories from videos in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision 
              and Pattern Recognition, pages 4700–4709, 2021</li>
            <li>Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as 
              neural radiance fields for view synthesis. Communications of the ACM, 65(1):99–106, 2021</li>
            <li>Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: 
              Large-scale learning and evaluation of real-life 3d category reconstruction. In Proceedings of the IEEE/CVF International Conference 
              on Computer Vision, pages 10901–10911, 2021</li>
            <li>Vincent Sitzmann, Michael Zollh ̈ofer, and Gordon Wetzstein. Scene representation networks: Continuous 3d-structure-aware neural scene 
              representations. Advances in Neural Information Processing Systems, 32, 2019</li>
            <li>Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. 
              IEEE transactions on image processing, 13(4):600–612, 2004</li>
            <li>Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the 
              IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4578–4587, 2021</li>
            <li>Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a 
              perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586–595, 2018</li>
          </ol>
        </div>
      </div>
    </div>
    <!--/ References. -->
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/yankeesong/adaptive-volume-rendering" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page is built based on the template from <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
